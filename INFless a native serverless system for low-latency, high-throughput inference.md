## INFless a native serverless system for low-latency, high-throughput inference



#### ABSTRACT

目前的许多网站逐渐依赖于ML来提高他们的业务效率。但由于开发和维护ML服务给开发者带来了巨大的开销，因此选择合适的方法来降低ML服务的开销十分重要。虽然serverless系统是降低开销的一个有希望的解决方式，但是经过调查，目前通用的serverless系统不能让ML服务以低延时、高吞吐的方式运行

由于通用的serverless系统不能完全解决ML服务开销过大的问题，作者认为适用于ML服务的serverless系统应当原生的将推理任务的特性与serverless系统的范例进行结合。因此，作者提出了第一个面向ML领域的serverless平台INFless。INFless具有如下优势：提供了统一的、异构的资源抽象(包括CPU和各类加速器)；利用内置batching和非均匀的扩展机制实现高吞吐量；通过协调管理batching排队时延、运行时延和冷启动比例实现低延时

作者经过本地测试和大规模仿真，检测到相比起目前最新的系统，INFless可以提供2×至5×的吞吐量，且能够满足ML服务的延迟需求



#### INTRODUCTION

由于serverless计算具有自动化资源管理、自动扩展和经济效益高等特点，使其在众多领域取得了巨大的成功，例如网络服务。而如今被广泛应用的ML推理服务，是采用serverless计算的另一个有前景的应用领域。与其它后端应用不同的是，ML推理通常会被融入到在线网站中，需要快速进行响应，具有严格的时延需求。虽然之前的许多研究都表明serverless平台很可能能成为ML推理服务的重要底座，但是目前已有的商业化serverless服务，如Amazon Lambda，都难以满足ML推理服务对时延严格的需求(平台不允许用户规定时延的SLO，使其难以适应多样的时延限制)，且不能合理利用资源实现高吞吐量(大型推理模型通常是高度平行化和计算密集的，适用于多种硬件加速器，但这些平台只提供CPU计算)

为了提升系统的吞吐量，目前的一些框架采用了将多个输入组织成批进行批处理的方法，从而更高效地运行。然而，这些方法采用的是On-Top-of-Platform (OTP)的设计，即在系统前端增加一层缓存buffer层，在缓存层将输入组织成批并派发的实例中运行。这种方法虽然能够提升经济效益和吞吐量，但同时也引入了额外的调度时延(这个应当不是调度时延，而应是将输入从buffer层转发到调度器的转发时延)，且无法保证端对端的时延。另外，OTP的设计使得buffer不能考虑到底层的资源分配情况，使其只能有限地优化系统吞吐量(如果模型的计算效率随资源配置和batch大小变化，OTP的方法难以管理系统的资源分配)

**考虑MArk与INFless的区别：**

**MArk是面向用户的一个推理服务系统，是由应用开发者自身进行维护和部署的。虽然在OTP的基础上，MArk也增加了自己的资源管理层，用于限制batching的排队时延等，但其终究需要通过申请云提供者提供的实例，来为推理服务赋能。因此，其对底层资源的调度和排队等是难以感知的，因为这些都是云提供者的系统来负责，用户不具备足够的控制力。MArk为用户提供了一个自行部署的推理服务系统的解决方案**

**与MArk不同的是，INFless是云提供商的层面为用户提供的推理服务系统，属于云提供商自行利用自己的云资源，为用户部署推理服务提供一个平台，因此它可以将batch决策与资源分配、请求调度等结合起来，因此它对端对端时延等具有更好的控制。INFless为云提供商提供了一个serverless平台的解决方案，使云提供商能够基于自己的资源，为用户提供推理服务系统，降低开发者在维护和管理服务系统上的开销**

作者在本文中提出了native设计的serverless推理系统，即将推理的特性完全地融入到平台设计中，使serverless系统变成为推理服务量身打造的系统。然而，这样的系统仍要面临一些挑战，包括

1. 实现低时延：在serverless平台中，函数调度时延、实例冷启动时延和批处理时延都会影响处理一条请求的端对端时延。合理地安排各个步骤的时间开销，才能够确保每条请求能够在规定时间内被处理
2. 实现高吞吐量：硬件加速器和内置批处理机制的引入给资源管理和函数调度带来了更高的复杂性。为运行的函数实例选择最优的硬件、batch配置，才能最大化系统的吞吐量
3. 实现低开销：整体系统设计应当是易用且高效的，且新加入的模块或修改都应当是轻量级的且易于使用

为了克服上述困难，本文提出了serverless推理系统INFless，将推理服务以BaaS的方式运行。INFless接收用户提交的推理模型代码，并自动化模型的部署且根据变化的负载进行扩展。INFless允许用户声明他们高层次的性能需求，如延迟SLO，并能够确保用户请求能在亚秒级的时间被处理。另外，通过native设计方式，即将推理服务的特性与serverless范例完全结合，INFless能够达到高的资源利用率和吞吐量。INFless先进性主要体现在

1. 将batch管理和异构资源分配的设计相结合，并提出了一个非均匀的扩展策略来提升资源利用率
2. 提出了一个轻量级的分析方法，从而快速地搜索到能够满足查询性能需求的实例配置
3. 采用LSTH策略进行预测，减少冷启动和资源开销
4. 在OpenFaaS平台上实现了INFless，并从实际部署和大规模模拟器评估中证明了它在资源效率和保障SLO方面有良好的表现



#### BACKGROUND & MOTIVATION

-  Serverless Inference

  近些年来，由于serverless计算技术快速发展，许多应用都被部署到一些商业化serverless平台上运行。其中，这些应用最重要的负载是网页服务。ML技术也被广泛地与网页服务进行结合，如中国最大型的生活类网站已经部署了超过600个ML推理模型，且每天需要处理超过百万条查询请求

  模型推理服务通常具有严格的时延上的限制，例如上述网站中的90%以上的推理模型都被要求在200ms内完成请求的处理。推理服务的特性使得部署推理服务的集群在资源管理和模型配置上给开发者带来了不可忽视的开销。在serverless系统部署推理服务能够带来如下优势：

  1. 推理服务可以容易地与前端应用进行解耦，并被部署为无状态的函数
  2. 开发者可以快速地使用函数模板构建起推理服务
  3. serverless系统的自扩展能力可以使其灵活应对负载变化(节省了开发者的集群管理成本)
  4. serverless系统采用的计价模型(pay-per-use)可以为服务提供者节约成本

  因此，将推理服务部署到serverless平台成为一个有希望的解决方案。

- Limitations of Existing Serverless Platforms

  本小节主要介绍已存在的serverless平台Lambda在部署推理服务方面的不足

  1. 高延迟：

     商业serverless平台通常需要开发者自行定义函数实例使用的内存大小，并配备相应比例的CPU供实例运行。作者测试了不同模型在不同的内存条件下处理请求需要的推理时延，其中×表示因内存过小导致模型无法载入到内存中，如下图

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220908165935078.png" alt="image-20220908165935078" style="zoom:50%;" />

     可以看到，对于部分小型的模型，如MNIST，只要它能装入内存中，则只需要很低的延迟来处理请求。而对于大型的模型，如Bert-v1，即使分配了最多的内存空间，受限于CPU的算力，其推理延时仍旧很高，没法满足200ms的时延SLO。造成部分模型时延过大的主要原因在于，商业serverless平台不支持各种硬件加速器来加速推理任务，因此无法为大型的推理模型提供低时延的推理服务

  2. 难以应用batching

     batching是为ML推理服务设计的重要优化方案。它可以通过将多个请求并行执行从而推理服务的资源利用率。作者在Lambda平台上测试了采用batching的情况下(batch大小为4或8)各个模型在不同内存条件下的推理服务时间，得到如下结果

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220908173050794.png" alt="image-20220908173050794" style="zoom:50%;" />

     可以发现，大部分的模型在采用batching方法时都面临较长的推理时延。特别对于一些在常规下能以较低时延进行推理的小型模型，如SSD、Textcnn-69，在采用batching后延迟增加了4×，超过了200ms的时延需求。可见对于小型模型，平台在batching下无法提供低延时的推理服务(大型模型在任何情况下时延都较高)

  3. 资源过量配置

     对于商业的serverless平台，它们采用的分配策略是依据用户定义的内存数量来等比例分配实例的CPU资源，供用户执行推理任务。然而，这种CPU-内存成比例的分配策略不适用于推理服务这类计算密集型任务，造成了巨大的资源浪费和很低的资源利用率。如下图，为了获得足够的CPU算力，用户往往需要为实例申请过量的内存资源(超过50%的内存资源是不被用到的)

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220908174011711.png" alt="image-20220908174011711" style="zoom:50%;" />

  4. 实例与请求一对一映射

     目前已有的serverless平台通常只支持实例与请求一对一映射，即每个实例一次只能处理一条请求。这种分派方式往往造成了低的资源利用率和创建出过多的实例，特别在负载突发性增加的情况。因此，应当采用batching的方式，将多条请求同时分派到一个实例上运行，减少实例的创建。如下图所示，将请求通过OTP的方法组织成批，能够大量降低函数调用次数(上)和实例的创建数量(下)

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220908180312544.png" alt="image-20220908180312544" style="zoom:50%;" />

  5. OTP的batching设计具有局限性(可以与Introduction中的缺陷对应)

     虽然OTP的batching方案通过提前将负载进行聚集并组织成批交给serverless系统处理，提升了serverless系统处理的吞吐量并降低了经济开销，但是它仍旧有以下的缺陷

     (1) OTP方案需要将其部署和维护在serverless系统外的系统，给开发者带来的管理开销

     (2) OTP方案无法感知到serverless系统内的调度时延和排队时延，使得它难以调整batch时间来满足时延需求(额外的调度时延，且无法确保端对端时延)

     (3) OTP方案只能调整Lambda系统外的实例配置参数，对于Lambda内部的分配和扩展方案是无法感知的。而Lambda内部采用的统一的扩展方案，无法在工作负载变化时自适应调整参数，使得负载分配和资源调度的决策不理想

     以下对比了INFless、不采用batching和OTP方案的对比，可以看到虽然OTP方案能够增加一定的吞吐量(30%)，但是INFless在吞吐量的影响上更大(3-4倍的增量)

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220908181345440.png" alt="image-20220908181345440" style="zoom:50%;" />

- Implications

  由于已有的serverless系统的局限性，人们期望能够开发出一个一个能够提供低延时和高吞吐量的serverless推理系统。基于已有的观察，这样的推理服务系统应当满足以下特征

  1. 支持异构的硬件(CPU和加速器的混合)，提高处理推理请求的效率
  2. 采用高效的资源分配策略，灵活地变更CPU、内存等资源的分配，提升资源的利用率和系统的吞吐量
  3. 支持内置的batching，将批处理与其它决策综合考虑，并使用灵活的扩展策略，在保证低延时的同时增加系统的吞吐量



#### SYSTEM DESIGN

以下介绍INFless的系统架构设计，INFless的系统架构和运行过程可以表示为下图

<img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220908204819485.png" alt="image-20220908204819485" style="zoom:50%;" />

INFless的基本思想在于将serverless推理平台与推理任务的特性相结合(利用推理任务共享操作符、计算密集型等特点)，以提高系统的性能。INFless的核心组件在于non-uniform scaling engine。其中non-uniform指的是来自相同函数的实例可能具有不同的配置，使得INFless能够在实例层级具备灵活分配资源的能力。Auto-scaling Engine以推理模型信息和集群资源状态作为输入，确定当前的实例配置(batch大小，CPU-GPU配置等)，从而在确保SLO的同时最大化系统的资源利用率和吞吐量

INFLess为开发者提供了函数模板，从而能够快速完成推理函数的构建。用户部署函数后，INFless通过结合函数的操作符信息，构建出函数的性能预测模型。性能预测模型的构建是轻量级的工作，因为它只需要解析出用户提交的推理模型的DAG结构(函数的调用结构)，并将运算符的运行时间相加，即可推测出总的推理延迟。由于推理模型通常采用同样的操作符集合，我们可以提前对这些操作符进行分析。提供的资源模板为(开发者在模板中定义使用的模型和对应的需求等)

**一个模型架构事实上就是一个类，而训练好的模型就是将给定的参数(模型的神经元数量，层数，以及训练出的超参数等)传到类中，形成一个特定的对象。一次推理工作事实上就是将输入传输给模型类，通过运行类内部定义好的函数得到对应的输出。通过分析模型的函数的调用流程和运算次数等，即分析推理模型的DAG，即可获得推理模型的性能预测模型。这里的操作符包括了卷积、池化、激活函数、加权和等操作，可以提前对这些操作的运行时间进行分析预测**

<img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220908223242362.png" alt="image-20220908223242362" style="zoom:50%;" />

在查询请求处理部分，INFless会自动根据当前的负载情况和资源情况生成合适的满足时延需求(应用开发者预先定义)的实例配置。当用户发送推理查询请求后，请求会被打包到内置的batch中并随后被分派给相应的实例。如果负载情况或集群资源状态发生变化，auto-scaling engine会依据非统一的扩展策略(non-uniform scaling policy)选择新的实例配置参数，从而在新的系统状态下实例能够满足应用需求。由于实例的冷启动会带来不可忽视的时延，INFless还设计了LSTH策略来减少冷启动的影响。综上，推理请求的处理时延包括了冷启动时延、在batch中等待调度的时延以及batch在实例中运行的时延



#### METHODOLOGY

- Built-in, Non-Uniform Batching

  在介绍这种Batching方法前，首先先介绍Built-in和Non-Uniform的概念

  1. built-in：batching操作被结合到了serverless系统中，使得可以同时且结合进行batch大小、资源分配和实例放置的决策

  2. non-uniform：non-uniform特指非统一，指不同实例具有自己的batch队列来聚合推理请求，且不同实例(不论是否来自同个函数)的batch大小、资源分配等都可能不同，使得实例可以利用上碎片的资源来保持高的资源利用率

  下面详细介绍具体的决策方式

  1. 单个实例的batching和请求分配

     对于单个实例而言，如果给它分配的请求过多，请求到达率太高，往往会导致batch很快被填满而剩余请求需要被丢弃的情况；如果分配的请求过少，则会导致一定时间内没办法填满batch，造成低的吞吐量。为了确保SLO需求，且提高系统吞吐量，为单个实例分配的请求到达速率需要严格控制在一个区间内，即[r low,r up]。上下界的计算方式为

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220909101945105.png" alt="image-20220909101945105" style="zoom:50%;" />

     其中b为当前的batch大小。为了确保r low小于r up，batch的提交间隔不应当低于一个batch的运行时间。r low的计算反映了在batch中等待时间+运行时间不能超过slo时延(这里假设冷启动能被避免)

  2. 整体的batching和请求分配

     设R为当前系统整体的请求到达率，且系统的请求到达上下界为Rmin和Rmax(将每个实例的上下界求和得到)，那么为每个实例分配请求的方式可以表示为

     - 如果R > Rmax，说明系统的请求到达情况已经大于系统最高吞吐量，则此时在令所有实例保持最高的请求分配数r max的同时，需要通知Auto-scaler进行实例扩展来处理剩余请求

     - 如果𝛼Rmin + (1 − 𝛼)Rmax< R < Rmax，此时我们可以控制α来在保持系统吞吐量的同时，不会在负载变化时频繁的伸缩(这个界限应该相对放宽，避免伸缩震荡，但α值也应适当降低，从而保持各个实例能接收到足够的请求，可以在实践中灵活调整α的值)。对每个实例可以按比例进行请求的分配，每个实例分配数量为

       <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220909103551619.png" alt="image-20220909103551619" style="zoom:50%;" />

     - 如果R < 𝛼Rmin + (1 − 𝛼)Rmax，则通知Auto-scaler释放部分的实例，来维持第二条状态

- Combined Operator Profiling

  本节旨在提出轻量级的COP分析方法，能够以低的开销和延迟构建出函数的性能预测模型，预测出函数实例在不同batch大小和资源配置下的推理延时，帮助Auto-scaler作出资源分配的决策来满足SLO(根据batching策略，分配的资源可能还需要使实例推理时间小于SLO/2，否则没有办法采用batch操作)

  - COP方法的提出主要来自于如下的发现：

    各种推理函数共享相同的操作符集合，且小部分的操作占用了实例大部分的推理时间。观察LSTM和ResNet-50两种模型实例操作符的调用和运行时间，可以发现LSTM的FusedMatMul和Matmul操作符占用了超过76%的时间，而ResNet-50的Conv2D操作符占用了95%的时间。因此，模型实例的运行时间可以根据主导的操作符进行预测(实际预测中考虑到了所有的操作符)

    <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220909112922845.png" alt="image-20220909112922845" style="zoom:50%;" />

  - COP方法的基础：分析各个操作符的运行时间

    对于给定的操作符o，将其描述为一个五元组o = <p,b,c,g,t>，其中p表示操作符的输入大小，b表示采用的batch大小，c表示CPU的配置(核数、存储带宽等)，g表示GPU的配置(内存、SM数量等)，t表示上述配置下的运行时间。通过预先将操作符在各种配置下的运行时间进行分析，并将运行时间记录在数据库中，可以加快对模型实例的推理时间分析(不需要再次离线分析各种操作符运算时间)。虽然各种配置项具有很大的选择范围，但是我们只需要选择几种重要的值进行分析即可，如batch大小只取2、4、8、16等

  - COP方法实现：根据模型的DAG进行时间分析

    根据提前分析出的操作符的运行时间，我们可以对模型实例在各种配置下的运行时间进行预测。我们可以根据推理模型构建出操作符的运算关系图G = (O,E)，其中O表示操作符集合，E表示操作符之间的先后关系(o1 -> o2存在一条边仅当o2需要接收o1的输出作为输入)。关系图可以被分解为两种不同的结构：操作符链(sequence chain)和并行分支(branches)。对于一个操作符链，其运行时间可以表示为所有涉及的操作符的运行时间总和；对于一组并行的分支，它由多条并行运算的链组成，并有相同的终点，其运行时间可以表示为并行运算的链的最大值。通过分析所有操作符链和并行分支的运算时间，我们可以最终估算出在某种配置下模型的推理时间

    **对关系图的分析应当可以直接采用拓扑排序的方式获得最终的运行时间，从形式上可以视为对运算关系图的拆分**

  - COP方法预测效果

    <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220909121018234.png" alt="image-20220909121018234" style="zoom:50%;" />

    通过对比多个模型的预测运行时间与实际运行时间，可以看到采用COP的方法进行预测能达到少于10%的平均误差。如上图，ResNet-50、MobileNet和LSTM-2365的平均误差分别为8.6%、7.8%和9.74%。特别地，对于LSTM-2365这种多种误差都接近于某个值的模型，可以直接将预测值提升10%来拟合实际的运行时间

- Scheduling

  Auto-scaling engine通过实时监控RPS并判断当前已有的实例是否足够处理这些请求。如果不够，Auto-scaling engine则启用调度算法来构建新的实例处理请求。在给定推理任务的性能预测模型、当前的系统资源状态和请求的到达率，Auto-scaling engine旨在让实例能够满足SLO的同时，最小化其运行资源

  - 问题描述

    下面对我们的问题进行数学化描述。假设当前集群中可用的机器数为m，需要运行n个实例(可以来自不同函数)。对于实例i，其资源配置描述为bi(batch大小)，ci(CPU数)，gi(GPU数)和xij(当实例i分配到机器j时，xij设为1)，如果bi为0则视实例没有被启动。此外，我们还定义yj，表示服务器j已用于实例部署。我们整体的目标和约束条件可以如下表示，我们逐一分析目标和约束的含义

    <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220910101237609.png" alt="image-20220910101237609" style="zoom:50%;" />

    (2) Cj、Gj表示机器j上可用的CPU和GPU数量，即只要机器j被使用，其未被利用的CPU和GPU部分也要被算入开销中。由于CPU和GPU的代价不同，我们在目标函数中增加了β，根据GPU和CPU的性能对比进行设置。我们的目标函数在于最小化所有服务器的总资源使用量

    (3)(4)约束表示对于任意实例，其batch等待时间和运行时间不能超过SLO约束。其中运行时间可以通过COP方法预测得到，等待时间可以由bi/ri获得，ri表示当前实例的请求到达速率

    (5)(6)约束表示对于运行在任意一台机器上的所有实例使用的资源总数不能超过该机器的资源总数

    (7)约束表示应确保任意函数k当前的实例的数量和配置能够服务所有到达该函数的请求(Rk)，且请求的到达率不会低于某个阈值，这个约束在batching部分有解释

  - Scheduling算法

    由于解决上述的优化问题是一个NP难问题，无法在多项式时间内解答，作者转而采用一个贪心调度算法来解决问题(算法中Rk表示函数k未能被处理的请求数)

    <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220910103417205.png" alt="image-20220910103417205" style="zoom:50%;" />

    由于batch的大小是增加系统吞吐量的关键因素，因此对batch大小的遍历是从大到小进行的，并且在固定batch大小的情况下来搜索可行配置。AvailableConfig函数在一个固定的batch大小下，搜索能够令实例满足SLO需求的可行配置，如果batch大小大于1时，我们要求实例的配置能满足(3)(4)约束。因此，我们筛选出texec<=tslo/2(运行时间小于SLO的一半，则最长等待时间可以超过运行时间)以及Rk>=rlow(剩余负载应能在规定时间内填满batch)的配置信息，并将其添加到可行配置集合中。如果batch大小为1，那么不会存在排队时延，因此运行时间小于SLO即可作为可行配置。如果AvailableConfig没能搜索到可行的配置，则会缩小batch大小重新进行搜索

    在算法的剩余部分，内容主要是在可行配置集合中选择出最符合要求的实例配置(满足大吞吐量的同时尽量减少资源碎片)，并启动相应的实例。为了衡量这个标准，文中提出了资源效率指标的概念，以说明资源配置i在机器j上的运行情况。指标eij计算方式为

    <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220910110438825.png" alt="image-20220910110438825" style="zoom:50%;" />

    根据eij的计算，我们选择最佳的<实例配置i，机器j>组合(eij的值最大)，并将采用配置i的实例运行在机器j上。我们更新函数k拥有的实例数、机器的资源剩余量、剩余需要处理的请求数等，并在接下来重新从最大的batch大小开始搜索实例配置

    **这个eij的定义方式还是比较容易理解的，在分子部分，通过将实例配置的吞吐量除以资源利用量，选择的是单位资源吞吐量最高的实例配置；分母部分，将资源使用量除以资源剩余总量，选择的是在分配资源后资源碎片最少的机器。将两个条件(吞吐量大，减少碎片)进行结合考虑，即可得到最优的组合方式**

- Managing Cold Starts with LSTH

  serverless系统中函数的冷启动往往会造成重大的性能影响。特别对于推理服务的函数而言，它需要将大型的模型和函数库加载到内存中，造成了很长的时延(可能比一次查询处理的时间更长)。因此，需要采取恰当的措施，减少系统中的冷启动次数，避免违背SLO。已有的方法hybrid histogram policy(HHP)通过跟踪一些参数来减少冷启动，但实验发现该方法过于保守，造成了较大的资源浪费

  根据调查发现(下图)，推理请求呈现出了两种与众不同的特性：从长期上看，推理请求的数量具有一定的周期性，用户的访问模型呈昼夜分布(LTP)；从短期来看，推理请求的数量存在大量的抖动(突发性增长和下降,STB特性)。基于HHP方法，作者提出了LSTH的策略来减少冷启动和资源开销(需要结合着HHP来理解)。然而，由于存在请求突发性增长(请求到达峰值超过历史峰值)，即使采用了LSTH，也存在正在运行的实例不足以服务请求的情况，因此冷启动是不可避免的。可以通过一些技术如SOCK和Catalyzer来减少冷启动的时间

  <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220910162122916.png" alt="image-20220910162122916" style="zoom:50%;" />

  

####  SYSTEM IMPLEMENTATION

本节讲述的是INFless在OpenFaaS上的实现过程(主要是通过修改平台上的调度器等模块，并将部分模块的内容进行更改，从而部署对应的INFless系统)



####  EVALUATION

本节主要记录的是evaluation中各个实验的设计、采用的数据集以及对比的baseline。本文的测试分为本地集群的测试以及大规模的仿真模拟

- 本地集群实验

  采用8-机器的集群进行实际测试

- 仿真模拟

  将集群从代码上(记录的资源数等)扩展到包含两千个服务器的集群，测试控制器的算法效率和INFless的开销。

- 采用的工作集

  通过使用如下模型，构建出两个适用的网络应用：线上二手交通工具交易平台(OSVT)和Q&A机器人服务

  <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220911094804074.png" alt="image-20220911094804074" style="zoom:50%;" />

  OSVT应用了SSD、MobileNet、ResNet-50等机器学习模型来提供物品识别、认证和交通工具分类等服务。Q&A采用合适的模型来理解用户问题并提供合适的回答，其SLO都被设为50ms。两类应用的batch大小都小于等于32。两类应用的动态调用通过使用来自Azure Functions的生产数据进行模拟

  (Serverless in the Wild: Characterizing and Optimizing the Serverless Workload at a Large Cloud Provider论文中对Azure Functions的数据集进行了完整的分析，论文中conclusion部分说明了数据位置，该数据集适用于对serverless的研究)

- 对比的系统

  本文将INFless系统与OpenFaaS+和BATCH进行了对比

  OpenFaaS是一个采用docker构建的serverless框架，作者对其进行了扩展，使其能够使用GPU运行实例。由于它不支持批处理，且采用统一的实例配置，因此将实例配置设定为2CPU、10% GPU SM，并设置实例具有固定的存活窗口300s

  BATCH是一个采用了OTP设计的serverless推理系统，采用batching技术来提升吞吐量

- 实验设计与结论

  1. 对比INFless、OpenFaaS和BATCH系统的吞吐量情况：可以看到无论是对稳定的负载，还是对Bursty的负载，INFless在吞吐量上都有很大的提升
  2. 分析组件对吞吐量的贡献情况：INFless主要采用三种组件来提升吞吐量，包括built-in batching(BB)、combined operator predicting(OP)和resource scheduling(RS)。分别取消这三类组件的功能(batch大小设为1、提供错误预测和采用简单的调度方法)，发现BB对吞吐量的影响最大
  3. 灵活性对比：INFless与BATCH相比，由于采用了non-uniform的调度策略，使其能够选择更多样的batch大小和实例配置，从而应对不同的工作负载，并可以用小的batch和配置填补资源，减少了资源碎片的情况
  4. 资源分配对比：与BATCH相比，INFless因其灵活的调度算法和采用LSTH减少冷启动，使其能够对资源变化快速反应，减少在请求陡降的时候仍然保留很多实例的情况，减少了资源浪费
  5. SLO违背情况对比：对于任意的负载，INFless相比BATCH和OpenFaaS都更少地违背SLO限制
  6. 冷启动次数对比：与HHP策略相比，LSTH的策略减少了20%的冷启动次数
