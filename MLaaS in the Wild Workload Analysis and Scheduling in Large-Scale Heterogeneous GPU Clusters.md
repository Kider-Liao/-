## MLaaS in the Wild: Workload Analysis and Scheduling in Large-Scale Heterogeneous GPU Clusters

#### Abstract

随着ML算法和技术的持续演进以及大规模可用数据中心的出现，许多企业已经开始在云上构建MLaaS来为ML任务提供底座和赋能，这些基础设施可能配备了异构的GPU。然而，在异构的GPU集群上运行多样的ML任务带来了不少的挑战。通过在阿里巴巴的产业化GPU集群上收集负载数据并进行研究，本文提出了GPU集群调度所面临的问题，包括低GPU利用率、长排队时延、需要高端GPU和具有严格需求的难以调度的ML任务的出现、异构机器上负载不均衡以及CPU造成的可能的性能瓶颈。本文已经对这些问题提出了一些目前的解决方案，并呼吁有更多的研究能帮助解决这些问题

(本文是一篇综述的文章，旨在通过数据收集和分析找到并提出目前存在的问题，即GPU调度存在的问题。在提出基本的解决方案的同时，文章更多的是提供了开放的思路，供后续工作的进行)



#### Introduction

问题背景：随着ML算法和技术的持续演进以及大规模可用数据中心的出现，ML任务的性能在许多实际应用上已经取得了重大的突破。为了加速大规模ML任务的运行，科技企业利用大量的GPU设备构建起了庞大且快速的并行基础设施，用于运行各样的ML任务，为MLaaS提供了运行底座。由于这些设施被多个用户共享，用于提高利用率和降低开销，资源的分配和任务的调度成为了基础设施必不可少的功能

做出的实验：作者在阿里云的大型GPU集群上进行负载跟踪，收集了大量的数据并进行表征研究。与之前的负载分析工作(只关注CNN、RNN在同构环境下的运行)不同的是，本实验跟踪的工作负载是训练和推理任务的混合，覆盖了大量的运行在多种ML框架的ML算法(例如transformer-based language models、CNN、RNN等)，运行在硬件和资源配置不同的异构机器上，且具有各种不同的资源需求、调度需求等。由于这些负载是在实际中收集到的用户提交的各种ML任务，且覆盖了不同类型和需求，因此具备重要的实际意义，所对应的研究也能真实反映实际出现的问题

得出的结论：这些大量的异构ML任务和GPU设备给资源分配和任务调度带来了巨大的挑战，使得基础设施难以实现高利用率和任务的快速完成。具体的挑战包括

1. Low utilization caused by fractional GPU uses(利用率低)

   根据观测到的数据，一个ML实例往往只能应用到GPU的一小部分(平均为0.042个GPU)，导致将整个GPU分配给单个ML实例的粗粒度分配策略会造成GPU利用率过低的情况。为了解决这个问题，可以通过时分复用GPU(共享GPU)的方式，将多个小规模的ML任务整合到同一个GPU上运行，使得GPU的利用率升高。GPU共享不会造成实例之间的严重干涉，因为在高利用率的GPU中，只有 4.5% ML任务，可能会在 SM 上竞争

2. Long queueing delays for short-running task instances(任务排队时延长)

   短期任务实例由于线头阻塞(长期任务长时间占用资源导致短期任务无法调度)，往往更容易面临长排队时延的问题。根据调查，9%左右的短期任务实例需要消耗超过它们运行时间的一半的时间来等待被调度。一个有效的办法是通过经验预测各种任务的运行时间，并依此进行优先级调度。目前已开发出的方法需要用户使用专门的框架，便于进行采集和预测，但是这是不现实的，因为用户可能会使用便于自己的ML任务运行的自定义的框架，导致了时间难以预测

   然而，根据研究结果，大部分的ML任务都是重复性的。如果通过细致的建模，我们可以以较低的误差预测出大部分重复性任务的运行时间，使得我们可以做出良好的调度决策。根据模拟数据，如果我们能够将短期任务优先执行(根据运行时间从小到大排序调度顺序)，我们可以将任务的平均完成时间降低63%以上

3. Hard to schedule high-GPU tasks(难以调度的任务)

   对于某些计算任务，它们需要占用完整的GPU，并且能够利用先进的GPU的一些特性实现大量的加速。这些苛刻的需求使得这些计算任务难以被调度。一个简单的解决方法是为这些计算任务预留部分高端的GPU资源，将复杂计算任务的调度与其他任务的调度进行区分，并采用GPU共享的方式将没有苛刻需求的任务整合到其他的GPU上运行。然而，这个问题的解决上仍然面临一些挑战(例如先进的GPU资源可能长时间不被使用，利用率低)，且没有受到太大的关注，期望能够有更好的方法解决问题

4. Load imbalance(负载不平衡)

   在实验中，作者观测到了实际情况下，异构集群上的负载调度存在负载不平衡问题。总体来看，具有先进GPU的机器的CPU和GPU占用率往往低于其他机器。另外，由于拥有更多GPU的机器上，分配给每个GPU的CPU较少，因此负载需要每个GPU配备更多的CPU来使得每个GPU具有高利用率(CPU数量不够导致GPU无法保持高利用率)，反之拥有较少GPU的机器上CPU资源往往是溢出的

5. Bottleneck on CPUs(CPU引发的瓶颈)

   运行在CPU利用率高的机器上的计算任务往往会因为对CPU资源的竞争导致运行时间增加，且实验表明即使是对 GPU 要求很高的工作负载也会受到 CPU 竞争导致的损失



#### Background(问题背景)

- Fast growing data and GPU demand(问题背景)

  为可扩展的ML工作提供基础设施支持在产业界的数据处理流程中愈发重要。随着时间的推进，ML任务对训练数据和GPU资源的需求越来越大(在操作通用ML平台的过程中观测到该结果)，使得ML任务需要被扩展分配到大量的GPU设备上进行计算。在实验中，最大的ML任务需要被调度到超过1000台GPU设备上执行，这在集群调度上给集群带来了重大的挑战

- Alibaba PAI(实验环境)

  为了满足ML任务不断增长的计算需求，阿里云推出了人工智能平台PAI，提供了涵盖整个ML流水线的多项服务，用于为广大开发者的ML任务提供底座和赋能，让开发者能够更高效、灵活且简便地利用ML技术完成各种任务。在用户提供应用代码和配置的需求信息后，由平台应用对应的框架将ML任务划分成多个作业task，并将task运行在一个或多个运行在不同机器的实例上。PAI以Docker容器作为实例的载体，简化了task的调度和运行。如下是用户在PAI上提交作业的一个简单流程

  <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220831111805490.png" alt="image-20220831111805490" style="zoom:50%;" />

- Trace analysis(相关工作)

  在GPU集群上运行多样的大规模ML负载带来了重大的挑战。对负载进行追踪分析是我们理解这些挑战并在系统优化方面获得新的理解的重要方法。然而，已有的在GPU集群上的负载分析具有大小受限、负载单调和集群没有异构性等问题，因此不能够充分揭示问题所在。例如，微软提出的Philly trace关注于单GPU运行的训练负载(负载被分配到多个GPU上已非常常见，且云平台上往往需要运行训练任务和推理任务)，没有说明GPU类型、CPU的规格等，使得其推导出的结论具有局限性

  

#### Workload Characterization

- Trace overview

  1. Trace information(信息涵盖的内容)

     本文发布的信息是对运行在最先进的ML算法和多种主流框架上的训练和推理任务的追踪，其中大部分的任务都需要多个GPU的支持。跟踪信息记录了工作负载在不同层级(job、task和instance)上的到达时间、完成时间、资源需求和使用情况(包括GPU、CPU、内存等资源)。同时，机器级的信息同样也包含在追踪信息中，涵盖了硬件规格、机器上设备资源的利用率(由守护进程向机器上的Linux内核和GPU驱动进行查询)等。另外，即使工作负载的实例以独立的方式运行在容器中，使其语义对外是不可感知的，作者仍然人工地分析了一些负载并收集了相应的数据，为其他工作提供线索。

  2. Jobs, tasks, and instances(三者的联系)

     在PAI中，用户通过提交job来执行自己的ML任务。对于每一个job，它可以被拆分为一到多个具有不同计算任务的task。而每一个task运行一到多个instance(Docker容器)来执行计算任务。对于同个task的所有instance，它们具有相同的资源需求，可能会被组合调度到不同的GPU上(可能需要并行进行计算)

  3. Heavy-skewed instance distribution(实例分布的严重不均)

     根据收集的task实例的累积分布函数，可以看到实例分布非常不均匀。接近77%的实例是由前5%的用户提交的(人均超过17.5k个实例)，而后50%的用户人均只提交了少于180个实例。

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220831124921886.png" alt="image-20220831124921886" style="zoom:50%;" />

  4. The prevalence of gang-scheduling(task普遍需要分布调度)

     分布式的ML任务往往需要被分布调度。根据调查数据，有接近85%的task实例需要被分布调度，且有超过20%的task需要被分布调度到超过100个GPU上。总体上看，这些需要分布调度的task的需求占了总GPU需求的79%，这种普遍性使得GPU的利用率难以提高

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220831125652073.png" alt="image-20220831125652073" style="zoom:50%;" />

     **gang-scheduling更准确的翻译应当是组合调度，即要求一个task的所有instance都调度成功后，整个task才能开始运行。这里所述的应当是每个实例占用一个GPU的情况，不考虑GPU sharing的问题，所以才会有某个task同时需要超过100个GPU运行实例的情况**

  5. GPU locality

     除了需要分布调度的task之外，某些task希望它的所有instance能被调度到同个机器的GPU上，即具有GPU局部性需求。虽然这种调度需求往往会延长它的调度时间，但是由于实例在同个机器的不同GPU之间通信，大大降低了通信开销，从而大量加速分布式计算任务

  6. GPU sharing

     PAI允许task实例通过分时的方式以较低的开销共享GPU，这使得用户可以请求部分的GPU资源(即一个实例不需要完整占用一个GPU)来运行任务，提高GPU的利用率

  7. Various GPU types to choose from

     PAI为用户提供了多样的异构GPU(包括NVIDIA Tesla T4, P100和其他类别的GPU等)并允许用户指定特定类型的GPU来运行他们的工作。但根据收集的信息，发现只有6%的任务需要运行在特定的GPU上，其他任务对GPU类别没有特定的需求

- Temporal Pattern(时序特征)

  1. Diurnal task submissions and resource requests(任务提交和资源需求的昼夜变化)

     下图中记录了一周内任务提交和资源请求的变化情况。可以看到任务提交和资源请求都存在昼夜变化的情况，且主要集中在早上。除此之外，凌晨同样也是任务提交的高峰期，但通常是小规模的计算任务，不需要请求过多的资源。除了昼夜变化外，工作日的任务提交和资源请求数量要稍微多于周末

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220831174541761.png" alt="image-20220831174541761" style="zoom:50%;" />

  2. Instance run-time in a wide range(实例的运行时间位于大的范围内)

     下图显示了实例运行时间的分布情况。本实验收集到的数据与Philly类似，即实例的运行时间主要分布在四个数量级的范围内。而本实验观测到的实例运行时间总体要比Philly观测到的要短

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220831180129483.png" alt="image-20220831180129483" style="zoom:50%;" />

  3. Non-uniform queueing delays(实例的排队时延的相关信息)

     实例的提交时间到实例开始运行的时间被视为实例的排队时延。与运行时间长的实例相比，运行时间较短的实例的周转时间中排队时延的占比更大，受到更严重的影响。如下图中可以看到，9%左右的短期实例需要超过一半的时间在等待被调度上，而只有3%的长期实例受到这样的影响

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220831180835820.png" alt="image-20220831180835820" style="zoom:50%;" />

     除了实例运行时间外，实例的排队时延还与它们对GPU的需求有关。c图中显示了如果实例能够支持GPU共享，即可以只使用部分的GPU来进行计算，那么它的排队时延将大大降低。90%的支持GPU共享的实例能够在497秒内被调度，而有10%的不支持GPU共享的实例需要超过1000秒才能被调度；d图中显示了实例对高端GPU的需求也会影响它的排队时延。对于需要高端的V100系列GPU的实例，其排队时延的90%线为13709秒，而运行在低端的各种GPU上的实例，其排队时延的90%线为360秒，且中位数也远小于需要高端GPU的实例

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220831181010115.png" alt="image-20220831181010115" style="zoom:50%;" />

- Spatial Pattern(空间特征)

  1. Heavy-tailed distribution of resource requests(请求集中化，分布不均)

     根据下述的a、b、c图，即CPU、GPU和内存的请求分布情况，可以看到三类资源的请求都是heavy-tailed，即对资源需求高的请求集中在少量的实例上，使得资源倾斜于这些实例(20%的实例请求了大量的资源，而80%的实例只请求了少量或中等的资源)

  2. Uneven resource usage: Low on GPU but high on CPU(利用情况不均)

     许多用户更倾向于请求比他们实际需求更多的资源，而这往往导致资源的低利用率，这一点可以在下述图中的虚线看出(使用少量资源的实例占据了大多数)，且资源请求的中位数远高于资源使用的中位数(请求为6 vCPU cores, 0.5 GPUs, 29 GiB memory；使用为1.4 vCPU cores, 0.042 GPUs,3.5 GiB memory)。除了用户请求大于需求外，GPU的利用率还受限于其他的资源，如CPU。ML实例虽然对GPU有较大的需求，但是由于CPU等资源运行速率低，且ML任务需要CPU预处理数据或等待内存传输数据等，导致GPU在大部分时间都处于空闲状态，等待这些资源完成工作

     除了资源利用低于请求造成利用情况不均外，由于有些实例可能会使用宿主机上的备用资源来完成工作，它们的资源使用量可能会高于它们请求，这个问题在CPU上更为明显(例如，为了达到GPU的高利用率，可能需要比请求更多的CPU来执行数据处理)。从d图可以看出，有接近20%的实例会使用超过请求数目的CPU来执行任务，而只有3%左右的实例会超量使用GPU

  <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220831185919614.png" alt="image-20220831185919614" style="zoom:50%;" />



#### GPU Machine Utilization

在分析工作负载的特点之后，接下来分析设备的利用情况

- Utilization of Compute Resources(计算资源的利用情况)

  首先分析GPU、CPU、内存和GPU内存这些计算资源的利用情况。实验集群中具有1,295个2GPU机器和519个8GPU机器，且2GPU机器的CPU/GPU比例比8GPU机器更高。由于这两类机器具有不同的配置，实验每隔一定时间分别收集这两类机器的计算资源利用率，并计算每个时间点上两类机器利用率的P50点和P90点(P50表示有百分之50的机器低于这个利用率，P90表示有百分制90的机器低于这个利用率)。通过对这些P50点和P90点进行汇总，得到对应的累计分布函数，如下图

  <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220831213005868.png" alt="image-20220831213005868" style="zoom:50%;" />

  根据上述结果，可以观察到的结论是：CPU和GPU相比于内存和GPU内存有更高的利用率；相比于8GPU的机器，2GPU的机器的CPU利用率更低，因为其CPU/GPU比例更高，为每个GPU服务的CPU更多；无论是8GPU机器还是2GPU机器，几乎在所有时间段，内存和GPU内存的P90点都在60%以下，说明我们的计算任务较少是内存密集型的

  与其他资源相比，GPU资源在不同时间点的变化范围更大。对于各个时间段的P90点，GPU的利用率从40%变化到100%，即在有些时间段，90%的GPU利用率低于40%，而有些时间段存在利用率到达100%的GPU。另外，GPU的P90点与P50点的变化曲线也有非常大的差异。这种大范围的变化主要源自于GPU的爆发性请求和使用，也源自于调度器优先将负载打包到同个GPU上运行，而不是进行负载均衡

- Low Usage of Network and I/O(网络和IO设备的低利用率)

  网络和IO设备的使用也是分布式ML系统的重要部分。实验中通过对具有不同带宽的机器进行网络输入速率的测量，发现绝大部分的机器的网络输入速率都不超过带宽(最大传输速率)的50%，说明网络的利用情况不理想。另外，通过对CPU运行情况进行测量，查看CPU在IO、用户态和内核态运行的时间比例，发现CPU在绝大部分时间都在用户态和内核态运行(等待IO的时间是执行时间的接近千分之一)，说明IO带宽在实际中也没有充分使用

  <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220831220403547.png" alt="image-20220831220403547" style="zoom:50%;" />



#### Opportunities for Cluster Management

在集群管理中，我们最主要的两个目标是：以高利用率使用GPU机器、尽快完成用户提交的任务。以下说明了实现这两个目标的大致思路

- GPU Sharing

  1. 问题描述：GPU在设计之初是不运行被共享的，它作为一个独立的不可分割的资源单位被分配给任务实例，使得每个GPU上独立运行着一个任务实例。虽然这种方法可以给实例带来强的隔离性，但是由于多数实例事实上只能利用部分的GPU来进行工作，导致了GPU的低利用率的问题

  2. 解决方法：为了解决这个方法，PAI集群的调度器允许多个任务实例以时分复用和空间复用的方式运行在同一个GPU上。在这种情况下，一个任务实例可以申请部分的GPU来执行工作，并确保能够被分配到相应比例的GPU内存(空间复用，需要时还可以使用未被分配的GPU执行任务)。然而，通过GPU内的计算资源不会进行分配(如SM)，而是通过时分复用的方式，依次为分配在同个GPU的任务实例工作

  3. 优势：通过GPU共享的方式，可以大幅的节约供应的GPU资源，提高GPU的利用率。通过下述结果可以看到，共享的方法(measured)与非共享的方法(simulated)相比，平均只用到了非共享的方法50%的GPU资源

  <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220901103839492.png" alt="image-20220901103839492" style="zoom:50%;" />

  4. GPU共享潜在的问题：由于在同个GPU内的实例并不会被划分对应的计算单元来运行，GPU共享可能会出现同个GPU内的实例竞争计算资源的问题。然而，根据作者收集的数据，GPU利用率在95%以上的情况只占所有情况的7%，且这些高利用率GPU中运行多个实例的比例平均也只有接近4.5%(主要的高利用率GPU都没有竞争关系)。因此推测在实际情况下，GPU共享不会造成严重的竞争问题

- Predictable Duration for Recurring Tasks

  了解(正确预测)ML任务实例的持续时间是做出更好的调度决策的关键。本节旨在提出一个能够调度容器负载且忽略负载内部语义的预测调度器，使集群能够更科学地进行负载调度

  1. 已有的预测方式：目前已有的调度器基于任务实例的运行流程(迭代次数、损失曲线和目标精度)和任务的运行速率预测任务实例的运行时间。然而，由于这些数据的采集需要特定的框架支持(如TensorFlow和PyTorch)，而现实中用户可能会使用不同框架、使用自定义版本的框架或是提交非迭代任务(如推理任务)，使得这种预测方式不适用于所有情况。另外，对运行在容器上的实例进行数据的采集也是较为困难的事情

  2. 重复性任务的普遍性

     即使调度器忽略了负载的运行流程，我们依然可以根据任务过去的运行情况对其运行时间进行预测，因为多数的任务都是重复性的。在实验集群中，可以通过收集任务的一些元数据(每次提交都是相同的)，如入口脚本、命令行参数等，并将这些元数据作为哈希键值与任务的标签(Group tag)、出现次数、运行时间等进行关联，使得我们可以对重复性的任务进行标记。通过这个方式，作者发现有65%的任务都重复运行了至少5次(图10)

     除了周期性的训练任务外，许多重复性任务是批量推理任务，它们通过将请求的数据进行汇集并利用数据集合通过一次批量推理得到多个请求对应的推理结果。这些批量推理任务由于有固定大小的数据batch和相同的运行流程，往往有可被预测的稳定的运行时间(图11)

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220901113322154.png" alt="image-20220901113322154" style="zoom: 33%;" />

  3. 重复性任务的时间预测

     由于重复性任务可能被不同用户以不同资源需求提交，且它的实例可能具有不同的运行时间，因此我们通过任务的用户名、Group tag和资源请求来进行运行时间的预测。以这三个属性作为输入，并利用CART算法和一个树回归器，我们可以对任务实例的平均运行时间进行预测。回归器对每棵树最多进行 10 次拆分，并使用平均绝对误差 (MAE) 作为拆分标准

     **这里涉及到了一个决策树方面的算法CART，可以感觉到在许多领域中，AI算法是不可或缺的一种解决方案，是否需要专门去学习一些经典的算法**

     为了评估预测方法的准确度，作者对超过5次重复运行的任务进行了测试，使用80%的任务来训练预测器并用20%的任务进行测试。实验通过采用不同的属性作为输入，检测在这些输入下预测器的预测时间与准确时间的差异，如下图。经过测试，可以看到Group tag是提升预测器精度的最重要的属性，如果将其与User和Resource进行结合，可以使预测器更准确地预测出运行时间

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220901121545593.png" alt="image-20220901121545593" style="zoom:50%;" />

  4. 预测的优势

     作者通过一个模拟实验来测试对任务实例运行时间的预测给调度带来的好处。测试中利用设计出的一个离散时间模拟器，并将在实际实验中收集到的数据(资源请求、到达时间、实际运行时间等)和预测的运行时间作为输入提交给离散时间模拟器(忽略实例被调度到GPU的时间)，对所有任务的到达和运行进行模拟。测试中采用了两种调度策略，即FIFO和最短任务优先SJF，并分别应用在不同的调度器上，测试在不同大小的集群上所有任务的平均运行时间，得到下图结果。其中SJF-Oracle调度器依据的是实际的运行时间，而其他三个SJF调度器依据的是预测的运行时间

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220901123214107.png" alt="image-20220901123214107" style="zoom:50%;" />

     根据实验结果，可以看到采用预测的方式和SJF的调度策略后，整体的实例运行时间降低了63–77%，且预测更加准确(使用更多属性作为输入进行预测)的调度器能够更大幅度地降低平均运行时间，说明通过更加精确的预测，我们可以更加接近最优的调度

     **根据操作系统的调度，除了SJF外，我们还有其他可能效果更好的调度方法，如高响应比优先调度算法等非抢占算法，这些方法是否有机会拥有比SJF更好的效果**



#### Challenges of Scheduling

本节通过对两类具有代表性的ML任务(这两种ML任务分别代表了高的GPU需求和较低的GPU需求)进行样例探究，描述调度异构性ML任务的复杂性。作者描述了在生产中部署的调度策略，即根据不同的请求和使用模式区分两种类型的任务

- Case Study of High-GPU Tasks

  在集群中，有一小部分任务需要先进的GPU来运行计算密集型实例。这些任务的目标是训练最先进的模型或是利用训练好的模型为业务关键型、面向用户的应用程序提供推理服务。这类任务会请求具有较高内存和先进硬件功能的GPU设备。这一类的任务包括

  1. NLP with advanced language models

     在集群中，有6.4%的任务是通过先进的模型来执行自然语言处理NLP，如BERT、ALBERT和NLNet。这些任务中，有73%具有大规模的输入且并须在具有16GB以上的内存的GPU上运行。以下对比了NLP任务(右)和其他常见任务(左)的GPU请求和使用情况，可以看到NLP任务有60%以上都请求超过1个GPU，且使用超过0.4个GPU，计算强度要远高于其他常见的ML任务

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220901164042974.png" alt="image-20220901164042974" style="zoom:50%;" /><img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220901164059472.png" alt="image-20220901164059472" style="zoom:50%;" />

  2. Image classification with massive output

     在集群中，有些分布式训练任务要求它们的工作实例(worker instances)要被调度到同一台机器上，从而能够使用高速的GPU-to-GPU互联网络(如NVLink)来大幅度提升性能。一个典型的例子就是训练一个能够将商品图片分类到大量的SPU的分类模型(如ResNet-100k，即将ResNet最后的输出层修改为具有100,000 个 SPUs 输出的 softmax 层)。这类任务的模型具有大规模的全相联层，训练时需要工作实例之间执行大规模的梯度更新的交换，使得实例通信变为了性能瓶颈，因此它们的工作实例需要采用高速的互联网络来进行通信。下图显示了高速互联网络对这类任务的影响

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220901170518105.png" alt="image-20220901170518105" style="zoom:50%;" />

     **一个手机的某个牌子和型号，如iPhone 5s，可被视为一个SPU**

- Case Study of Low-GPU Tasks

  集群中运行的大部分任务都只有较小的GPU需求和使用率。作者通过分析三个热门的任务的运行情况，了解到这些任务花费了相当多的时间在CPU上，等待CPU完成数据处理和仿真，使得GPU利用率低下

  1. CTR prediction model training and inference

     在跟踪的所有任务中，有超过6.7%的任务执行的是广告点击率(CTR)预测。这些任务采用多样的模型，如DeepFM、DCN，并使用25%的实例进行模型训练，75%的实例提供推理服务。根据测试结果，这类任务无论是训练实例还是推理实例，都只有很低的GPU利用率(超过75%的实例GPU利用率只有0.1，推理实例的CPU利用率更高因为它需要持续地处理到来的数据)。作者还分别对三个分别利用DeepFM，DCN和DNN的推理实例的运行情况进行了检测，发现这些实例80%以上的时间都在运行CPU，从而获取并处理下一个输入批次，只有少部分的运行时间在GPU和IO上

     由于这些实例具有高的CPU使用率，它们更有可能与其他位于同个机器上的工作负载产生冲突(特别是在CPU利用率高的负载上)。作者通过在一个48核的机器上运行一个需要8个vCPU的DeepFM实例，并增加其他的负载来提升CPU负载(这些负载与测试实例不存在核的冲突)，来验证高CPU占用率产生的后果。可以看到虽然这些增加的负载的CPU使用不与DeepFM实例冲突，但在CPU占用率高的机器下，DeepFM的运行速率还是下降了，原因是它与其它负载在cache、memory bandwidth等资源上存在竞争

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220901173629507.png" alt="image-20220901173629507" style="zoom:50%;" />

  2. GNN training

     图像神经网络训练也是集群中的重要任务，其占用了集群中2%的实例，包括GraphSage、Bipartite GraphSage、GAT等。图像神经网络在训练之前，其输入图像需要经过一系列的预处理，包括EdgeIteration，NeighborSampling和NegativeSampling等。而目前而言，这些预处理在CPU上执行更具备成本效益，使得图像神经网络的训练也需要在CPU上花费大量的时间。下面实验结果显示了CPU利用情况和占用率，可以看到CPU仍旧占据了相当一部分的运行时间

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220901175859494.png" alt="image-20220901175859494" style="zoom:50%;" />

  3. Reinforcement learning

     集群中同样也运行着许多强化学习任务。一个RL算法迭代地利用CPU模拟生成一系列数据并利用这些生成的数据在GPU上改进学习策略。多数的RL任务都具有多个组合调度的实例(最多甚至有超过1000个，如图a)，而这些实例大多数都在运行模拟操作。这使得实例占用了大量的CPU和网络带宽，却只使用了少部分的GPU。下面的实验结果显示了实例在GPU上的低利用率(图b)

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220901181430256.png" alt="image-20220901181430256" style="zoom:50%;" />

- Deployed Scheduling Policies

  与对GPU需求较低的任务相比，GPU高需求的任务有更苛刻的调度需求且通常由关键业务应用提交，因此它们需要与其它的任务进行区分，优先被调度

  1. 采用的调度策略：Reserving-and-packing

     在集群中，调度器通过reserving-and-packing的策略进行实例调度，即为GPU高需求的任务预留高端的GPU，而尽量将其它负载打包到较为落后的GPU上运行。在利用多项任务特性，如任务并行度、采用的ML模型、类似任务的历史信息等建立好任务的性能模型后，调度器利用这个性能模型分析得到每个任务的计算效率。对于计算效率高于某个阈值的任务，调度器视其为GPU高需求任务

     对于每个任务，调度器都会为其设定一个排序的调度计划序列，每个调度计划都与一个GPU和一个超时阈值相关联。在调度一个任务时，调度器依据这个调度计划序列依次尝试每个调度计划，等待调度计划对应的GPU可用。如果当前调度计划对应的GPU在超时阈值后仍旧不可用，则调度器接着尝试下一个调度计划。对于GPU高需求的任务，调度器会优先检验高端GPU的可用性，即序列前面的调度计划关联的是高端GPU，而对于其它任务，其调度序列则相反

  2. 其它调度考量：Load-balancing

     除了区分GPU高需求任务和GPU低需求任务外，由于负载过度的机器往往会导致任务资源竞争和运行干扰等问题，因此在具有相似规格的机器之间保持负载均衡也很重要。因此，在Reserving-and-packing的基础上，还应当优先考虑负载较低的机器，其中机器的负载由CPU、GPU、内存的分配情况来衡量(资源的分配率)

  3. 采用的调度策略的优势(为什么需要Reserving-and-packing)：

     对于本实验调度器的调度策略，Reserving-and-packing的考虑要优先于Load-balancing，并有良好的改善效果。为了验证这一结论，作者采用一个模拟器，并对调度器分别利用两种调度策略(只采用Reserving-and-packing和只采用Load-balancing)，查看在给定样本负载输入下各负载的排队时延。根据结果，可以看到虽然90%以上的实例都几乎不需要排队时延即可被调度，但采用Reserving-and-packing的调度策略的平均排队时延相对Load-balancing策略降低了45%，原因是因为Reserving-and-packing的调度策略减少了长排队时延的实例(下图中R&P相比于Balanced更早达到100%)。而相比两种调度策略在关键业务实例上的调度表现，可以看到R&P策略相对Balanced策略降低了68%，验证了Reserving-and-packing的考虑应当优先于Load-balancing

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220901210247268.png" alt="image-20220901210247268" style="zoom:50%;" />

     **虽然Reserving-and-packing的考虑应当优先于Load-balancing，但是Load-balancing策略也是必不可少的，因为它是降低实例完成时间的重要方式，能够减少在同一台机器上运行的实例的冲突**

- Open Challenges(仍然存在的问题)

  1. Mismatch between machine specs and instance requests

     从实验中，作者观测到机器标准与实例需求不匹配的问题。对于8GPU机器和2GPU机器，它们能够被分配到的vCPU数目不同。在8GPU机器中，每个GPU可被分配到12个vCPU，而在这些机器上运行的实例需要给每个GPU配置22.8个vCPU来满足计算需求。反之，在2GPU机器中，每个GPU可被分配到38.1个vCPU，而实例只需要18.1个来完成运行，导致了过量配置的情况。这种情况导致的问题是，对于8GPU机器，其GPU利用率受限于CPU数量，无法达到高利用率，CPU利用率持续较高；而对于2GPU机器，即使其GPU被大量占用，但是CPU的利用率严重不足

     作者机器标准与实例需求的不匹配是可以被解决的，因为整个集群的CPU-to-GPU标准与用户需求接近(23.2和21.4)。相信这个问题可以通过更好的调度手段进行改善甚至避免(例如将高CPU的任务提交到2GPU机器，而高GPU的任务提交到8GPU机器)

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220902095035664.png" alt="image-20220902095035664" style="zoom:50%;" />

  2. Overcrowded weak-GPU machines

     与配备先进的GPU的机器相比，配备较为落后的GPU的负载过高(平均有77%的CPU和74%的GPU被分配)。这主要源自于使用的Reserving-and-packing调度策略，即优先把较落后的GPU分配给低GPU需求的任务，但这些任务在集群中占据了相当大的比例

  3. Imbalanced load in high-end machines

     与其它的机器相比，配备了先进的GPU的机器往往负载更低，其平均有35%的CPU和49%的GPU被分配，造成了低利用率的问题。造成这个结果的原因是这些机器通常是被预留给重要的高GPU需求任务，但这些任务只占据了所有任务的一小部分。除此之外，在配备了先进GPU的机器之间，也出现了负载不均衡的问题，如下图中具有V100GPU的机器上，位于下面的机器负载比位于上面的机器更大，说明了目前的负载分配算法仍有较大的可改进的空间

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220902100416761.png" alt="image-20220902100416761" style="zoom:50%;" />

  4. CPU can be the bottleneck

     根据之前的研究，大部分的ML任务都花费更多的时间在CPU上(CPU的使用强度高于GPU)。这些任务如果被分配到CPU利用率高的机器上，它们的运行时间很有可能被延长。为了验证这个结果(CPU竞争与实例减速的相关性)，作者将任务根据运行时间划分为accelerated、normal和delayed三类，并收集它们的运行信息和机器的使用状态。可以看到运行delayed任务的机器有更高的CPU占用率，但在GPU利用率上运行三类任务的机器没有明显差别

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220902101631589.png" alt="image-20220902101631589" style="zoom:50%;" />

     为了进一步验证CPU竞争与实例减速的相关性，作者追踪了热门的CTR预测任务(高CPU需求)的运行情况。观察实验结果，发现CPU利用率超过24%的机器运行了接近50%的delayed任务，却只运行了10%的accelerated任务，很好地说明了减速与CPU竞争有很强的关联性。相比之下，GPU的利用率与任务的运行速度没有明显的关联性。

     <img src="C:\Users\pentakill\AppData\Roaming\Typora\typora-user-images\image-20220902102121968.png" alt="image-20220902102121968" style="zoom:50%;" />

     总而言之，GPU集群中的任务实例调度也应该考虑到CPU竞争造成的干扰，要求我们能够设计一个结合考虑CPUs、GPUs、memory、I/O和网络等众多因素的资源调度器



#### Discussion

- Support of elastic scheduling(弹性调度是否能够带来益处)

  在异构GPU集群下，调度器面临的一个重要问题在于分布式训练任务要求其实例被组合调度，即同时给实例分配资源，使它们能够同时启动运行，使得这些任务难以被调度。为了帮助解决这个问题，一些框架进行了改进，支持训练任务能动态调整运行时的worker实例数量，从而允许弹性调度。这类训练任务可以先占用较少的资源启动，并在机器变得空闲时进行扩展，获取更多的计算资源，这使得允许弹性调度的任务更容易被调度和启动。然而，弹性调度往往给最终模型的精确性带来了不确定性

- Machine provisioning and resource disaggregation(资源分解是否是未来方向)

  根据之前的讨论，调度器还应考虑机器配置：8GPU机器可以提供庞大的GPU算力，而2GPU机器更适用于CPU密集型任务。为了简化这个问题，许多系统工作建议将单片机分解为一系列分布的、解耦的硬件组件，从而使得硬件能够被更加弹性地调度使用(虽然这种方法会带来不小的通信开销)。目前，TensorFlow在框架层面为这个提议做出了尝试。它通过将CPU层面的数据预处理与GPU层面的训练任务进行解耦，从而避免CPU的瓶颈问题(可以将预处理分配给大量的独立CPU)。但这种方法需要用户大量修改代码



#### Related Work

- GPU sharing

  1. 在硬件层面，英伟达的Multi-Instance GPU特性允许将一个大的GPU通过分配内存和带宽，划分为多个小的GPU实例。但是这个特性目前只在最新的 A100 GPUs支持，且无法随意地对GPU进行划分
  2. 在软件层面，可以使用CUDA提供的API对GPU进行时分复用，达到GPU共享的目的。但是这种方法带来了不可忽视的上下文变换开销，且不能提供良好的隔离性
  3. 在框架层面，可以通过对标准框架进行扩展，使它们支持细粒度的GPU共享，并为每个运行的实例以较低的开销管理GPU内存。这种方法往往需要用户进行代码修改，适应框架

- GPU cluster scheduler

  目前已有许多应用在集群上的调度器被发布，如Optimus、Tiresias目标在于最小化任务的完成时间，Themis、Gandivafair、HiveD考虑训练任务完成时间的公平性。但这些调度器都没有采用GPU共享，且它们能够被应用的场景是有局限的(负载种类、集群大小等受限制)

- ML workload characterization

  除了计算情况外，实例之间的通信以及IO对于分布式训练而言也是非常重要的，例如有些任务需要花费大量的时间来载入数据。有些ML调度器通过考察不同网络带宽下的训练效率并尝试通过缩减通信开销的方式来加速训练任务(利用GPU局部性等特性)。



#### Conclusion

在本篇文章中，作者分析了在阿里云PAI的大型GPU集群上追踪的ML任务的数据，包括了训练任务和推理任务。从整体上看，大部分的任务都具有需要组合调度的实例，且它们是重复性的，会被反复执行。从任务的需求上看，多数的任务都是小型的，它们的实例通常只需要少于一个GPU即可执行计算。这类任务的瓶颈主要在于执行数据预处理和模拟的CPU(CPU需求大于供给)；而有一小部分业务关键型任务需要配备了高速通信网络NVLinks的高端GPU，从而降低通信延迟，提升计算效率。在实验过程中，为了更好地调度PAI的工作负载，集群的调度器采用了GPU共享和reserving-and-packing的策略(为业务关键型任务预留高端GPU)，以提升资源利用率并降低排队时延。最后，本篇文章提出了一些仍未解决的问题，包括异构环境下负载均衡器的优化和CPU竞争导致的性能瓶颈等，这些问题期望在后人的研究中能有解决方式
